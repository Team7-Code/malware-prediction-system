import pandas as pd
import numpy as np
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_selection import chi2, SelectKBest
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

from optimize_file import *
import xgboost as xg
import datetime


def savePickle(file, name):
    name = name+str(datetime.datetime.now().date())
    file.to_pickle("../data/" + name + ".pkl")


# import dask.dataframe as dd

chunksize = 10000
for chunk in pd.read_csv("../data/train.csv", chunksize = 50000):
    sample = chunk
    break
# X_train.to_csv("../data/reduced_train.csv")
# train = pd.read_csv("../data/train.csv")

train = pd.read_pickle("../data/train.pkl")
train = optimize(train)

discard = ['Census_ChassisTypeName','Census_FirmwareVersionIdentifier','Census_InternalBatteryNumberOfCharges','Census_InternalBatteryType','Census_InternalPrimaryDiagonalDisplaySizeInInches','Census_InternalPrimaryDisplayResolutionHorizontal','Census_InternalPrimaryDisplayResolutionVertical','Census_PrimaryDiskTotalCapacity','Census_ProcessorClass','Census_ProcessorManufacturerIdentifier','Census_ProcessorModelIdentifier','Census_SystemVolumeTotalCapacity','Census_TotalPhysicalRAM','CityIdentifier','DefaultBrowsersIdentifier','IeVerIdentifier','SmartScreen']

train = train.drop(discard, axis=1)

def memory_usage(df):
    return(round(df.memory_usage(deep=True).sum() / 1024 ** 2, 2))
print('Memory used:', memory_usage(train), 'Mb')


# train.to_pickle("../data/train.pkl")

# correlation.to_pickle("../data/correlation.pkl")

y = pd.DataFrame(train.HasDetections)
y = y.astype("bool_")
X_train = train.drop("HasDetections", axis = 1)


# X_train.to_pickle("../data/X_train_backup.pkl")
# X_train.to_pickle("../data/X_train.pkl")
# y.to_pickle("../data/y.pkl")
# X_train_cat.to_pickle("../data/X_train_cat.pkl")

X_train = pd.read_pickle("../data/X_train.pkl")
y = pd.read_pickle("../data/y.pkl")

# 8921483
size = X_train.shape[0]
correlation = X_train.corrwith(y['HasDetections'])

cols_na_count= X_train.isna().sum()
cols_na_count = cols_na_count.to_dict()
cols_to_discard = {k: v for k, v in cols_na_count.items() if v/size > 0.80}
cols_to_discard = list(cols_to_discard.keys())
X_train = X_train.drop(cols_to_discard, axis = 1)
cols_na_count= X_train.isna().sum()
cols_na_count = cols_na_count.to_dict()
cols_na_count= {k: v for k, v in cols_na_count.items() if v != 0}

upper = correlation.where(np.triu(np.ones(correlation.shape), k=0).astype(np.bool))
to_drop = [col for col in upper.columns if any(upper[col]) > 0.95]


X_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=0.2, random_state=123)

ohe = OneHotEncoder(sparse=False)

ohe.fit(X_train)

cross_val_scores = cross_val_score(pipeline, X_train, y_train, scoring="roc_auc", cv=3)

xg_cl = xg.XGBClassifier(objective="binary:logistic",n_estimators=10)
xg_cl.fit(X_train,y)
X_train = X_train.drop(("MachineIdentifier"), axis=1)


X_train_cat = X_train.select_dtypes('category')
for col in X_train_cat.columns:
    X_train_cat[col] = X_train_cat[col].cat.codes
chi_stat = chi2(X_train_cat, y)
discard_features = []

for col, pval, score in sorted(zip(chi_stat[0], chi_stat[1], X_train_cat.columns)):
    if pval > 0.01:
        discard_features.append(score)
    print(col," ", pval," ", score)

# Discarding on chi results
X_train = X_train.drop(discard_features, axis=1)
X_train_cat = X_train_cat.drop(discard_features,axis=1)
# savePickle(X_train, "X_train_")
# savePickle(X_train_cat, "X_train_cat_")

X_train_discrete_columns = X_train.columns[(X_train<0).any()]
X_train_discrete = X_train[X_train_discrete_columns]
X_train_discrete = X_train_discrete.drop("MachineIdentifier", axis=1)
X_train_discrete = X_train_discrete.astype("category")

for col in X_train_discrete.columns:
    X_train_discrete[col] = X_train_discrete[col].cat.codes


chi_stat = chi2(X_train_discrete, y)
discard_features = []

for col, pval, score in sorted(zip(chi_stat[0], chi_stat[1], X_train_discrete.columns)):
    if pval > 0.01:
        discard_features.append(score)
    print(col," ", pval," ", score)

X_train_discrete = X_train_discrete.drop(discard_features, axis = 1)
X_train = X_train.drop(discard_features, axis=1)

dab_U = {}
for col in train.columns:
    if "UNKNOWN" in train[col].value_counts():
        dab_U.update({col:str(train[col].value_counts()["UNKNOWN"])})
    if -1 in train[col].value_counts():
        dab_U.update({col:str(train[col].value_counts()[-1])})